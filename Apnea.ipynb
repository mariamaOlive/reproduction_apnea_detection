{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Reproducibility study of Apnea Detection\n",
        "\n",
        "Group 2:\n",
        "- Ernest Onifade\n",
        "- Harry Setiawan Hamjaya\n",
        "- Mariama Oliveira"
      ],
      "metadata": {
        "id": "_cQbTs8877oA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgvD_aSv1O7x"
      },
      "source": [
        "This notebook is a reproducibility study of the paper \"Toward sleep apnea detection with lightweight multi-scaled fusion network.\" by Chen et al. The main goal of the paper is to develop a lightweight model able detect apnea based on single lead ECG recordings.\n",
        "<br>\n",
        "<br>\n",
        "Link to the code of the original study : https://github.com/Bettycxh/Toward-Sleep-Apnea-Detection-with-Lightweight-Multi-scaled-Fusion-Network/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing libraries"
      ],
      "metadata": {
        "id": "mvViOZD-99xT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cm5-rq9S8MQ",
        "outputId": "b5bb40cf-20cd-4f6c-e3d5-68f6e18b6b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biosppy\n",
            "  Downloading biosppy-1.0.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from biosppy) (0.22.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from biosppy) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.11.3)\n",
            "Collecting shortuuid (from biosppy)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from biosppy) (1.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from biosppy) (4.8.0.76)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->biosppy) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->biosppy) (3.2.0)\n",
            "Installing collected packages: shortuuid, biosppy\n",
            "Successfully installed biosppy-1.0.0 shortuuid-1.0.11\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.11.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2023.7.22)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n"
          ]
        }
      ],
      "source": [
        "! pip install biosppy\n",
        "! pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sxTBMhlaSdKI"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import sys\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "import biosppy.signals.tools as st\n",
        "import numpy as np\n",
        "import os\n",
        "import wfdb\n",
        "from biosppy.signals.ecg import correct_rpeaks, hamilton_segmenter\n",
        "from scipy.signal import medfilt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading dataset"
      ],
      "metadata": {
        "id": "q1cTyp1_-ERF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksAbpE-YTlGP",
        "outputId": "e89322c2-8aad-4f52-a410-0be4a17cb909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-24 20:10:45--  https://physionet.org/static/published-projects/apnea-ecg/apnea-ecg-database-1.0.0.zip\n",
            "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
            "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 608841016 (581M) [application/zip]\n",
            "Saving to: ‘apnea-ecg-database-1.0.0.zip’\n",
            "\n",
            "                      1%[                    ]   7.19M   941KB/s    eta 10m 28s"
          ]
        }
      ],
      "source": [
        "!wget https://physionet.org/static/published-projects/apnea-ecg/apnea-ecg-database-1.0.0.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76LPhggMT7m9"
      },
      "outputs": [],
      "source": [
        "!unzip apnea-ecg-database-1.0.0.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "In this step, two main tasks were performed:\n",
        "\n",
        "- Applying filters to the ECG signal in order to remove the noise\n",
        "- Feature extraction: R peak amplitudes and R‐R intervals\n"
      ],
      "metadata": {
        "id": "e9RU74kb-Qn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining functions and applying tranformation to the training dataset"
      ],
      "metadata": {
        "id": "3tzWkxmr_2bF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZfdFvEWS4Um"
      },
      "outputs": [],
      "source": [
        "# PhysioNet Apnea-ECG dataset\n",
        "# url: https://physionet.org/physiobank/database/apnea-ecg/\n",
        "base_dir = \"apnea-ecg-database-1.0.0\"\n",
        "\n",
        "fs = 100\n",
        "sample = fs * 60  # 1 min's sample points\n",
        "\n",
        "before = 2  # forward interval (min)\n",
        "after = 2  # backward interval (min)\n",
        "hr_min = 20\n",
        "hr_max = 300\n",
        "num_worker = 35\n",
        "# num_worker = 35 if cpu_count() > 35 else cpu_count() - 1  # Setting according to the number of CPU cores\n",
        "\n",
        "\n",
        "def worker(name, labels):\n",
        "    X = []\n",
        "    y = []\n",
        "    groups = []\n",
        "    signals = wfdb.rdrecord(os.path.join(base_dir, name), channels=[0]).p_signal[:, 0]\n",
        "    for j in tqdm(range(len(labels)), desc=name, file=sys.stdout):\n",
        "        if j < before or \\\n",
        "                (j + 1 + after) > len(signals) / float(sample):\n",
        "            continue\n",
        "        signal = signals[int((j - before) * sample):int((j + 1 + after) * sample)]\n",
        "        signal, _, _ = st.filter_signal(signal, ftype='FIR', band='bandpass', order=int(0.3 * fs),\n",
        "                                        frequency=[3, 45], sampling_rate=fs)\n",
        "        # Find R peaks\n",
        "        rpeaks, = hamilton_segmenter(signal, sampling_rate=fs)\n",
        "        rpeaks, = correct_rpeaks(signal, rpeaks=rpeaks, sampling_rate=fs, tol=0.1)\n",
        "        if len(rpeaks) / (1 + after + before) < 40 or \\\n",
        "                len(rpeaks) / (1 + after + before) > 200:  # Remove abnormal R peaks signal\n",
        "            continue\n",
        "        # Extract RRI, Ampl signal\n",
        "        rri_tm, rri_signal = rpeaks[1:] / float(fs), np.diff(rpeaks) / float(fs)\n",
        "        rri_signal = medfilt(rri_signal, kernel_size=3)\n",
        "        ampl_tm, ampl_siganl = rpeaks / float(fs), signal[rpeaks]\n",
        "        hr = 60 / rri_signal\n",
        "        # Remove physiologically impossible HR signal\n",
        "        if np.all(np.logical_and(hr >= hr_min, hr <= hr_max)):\n",
        "            # Save extracted signal\n",
        "            X.append([(rri_tm, rri_signal), (ampl_tm, ampl_siganl)])\n",
        "            y.append(0. if labels[j] == 'N' else 1.)\n",
        "            groups.append(name)\n",
        "    return X, y, groups\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    apnea_ecg = {}\n",
        "\n",
        "    names = [\n",
        "        \"a01\", \"a02\", \"a03\", \"a04\", \"a05\", \"a06\", \"a07\", \"a08\", \"a09\", \"a10\",\n",
        "        \"a11\", \"a12\", \"a13\", \"a14\", \"a15\", \"a16\", \"a17\", \"a18\", \"a19\", \"a20\",\n",
        "        \"b01\", \"b02\", \"b03\", \"b04\", \"b05\",\n",
        "        \"c01\", \"c02\", \"c03\", \"c04\", \"c05\", \"c06\", \"c07\", \"c08\", \"c09\", \"c10\"\n",
        "    ]\n",
        "\n",
        "    o_train = []\n",
        "    y_train = []\n",
        "    groups_train = []\n",
        "    # print('Preprocessing for train dataset...')\n",
        "    print('Training...')\n",
        "    with ProcessPoolExecutor(max_workers=num_worker) as executor:\n",
        "        task_list = []\n",
        "        for i in range(len(names)):\n",
        "            labels = wfdb.rdann(os.path.join(base_dir, names[i]), extension=\"apn\").symbol\n",
        "            task_list.append(executor.submit(worker, names[i], labels))\n",
        "\n",
        "        for task in as_completed(task_list):\n",
        "            X, y, groups = task.result()\n",
        "            o_train.extend(X)\n",
        "            y_train.extend(y)\n",
        "            groups_train.extend(groups)\n",
        "\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvkZ08gs_bGX"
      },
      "outputs": [],
      "source": [
        "# https://drive.google.com/file/d/199Hj2CpUMg-W6__fE7wB-0edcjMnVtSu/view?usp=sharing\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# answer_id = '199Hj2CpUMg-W6__fE7wB-0edcjMnVtSu'\n",
        "\n",
        "# answer_txt = f'https://drive.google.com/uc?id={answer_id}'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying transformation to the test dataset"
      ],
      "metadata": {
        "id": "Q5MYJ3M9_wdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtttZWhNTdZ-",
        "outputId": "8078e8ac-52f2-4b06-8942-67491c8f0d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "x11:  81%|████████▏ | 372/457 [46:40<10:49,  7.65s/it]\n",
            "x24: 100%|██████████| 429/429 [49:26<00:00,  6.92s/it]\n",
            "x06: 100%|██████████| 450/450 [52:33<00:00,  7.01s/it]\n",
            "x18: 100%|██████████| 459/459 [54:07<00:00,  7.08s/it]\n",
            "x22: 100%|██████████| 482/482 [55:20<00:00,  6.89s/it]\n",
            "x35: 100%|██████████| 483/483 [55:29<00:00,  6.89s/it]\n",
            "x29: 100%|██████████| 470/470 [56:06<00:00,  7.16s/it]\n",
            "x05:  90%|█████████ | 456/505 [56:24<04:55,  6.03s/it]\n",
            "x04: 100%|██████████| 482/482 [58:09<00:00,  7.24s/it]\n",
            "x07: 100%|██████████| 509/509 [58:37<00:00,  6.91s/it]\n",
            "x33: 100%|██████████| 473/473 [58:44<00:00,  7.45s/it]\n",
            "x34: 100%|██████████| 475/475 [58:51<00:00,  7.43s/it]\n",
            "x19: 100%|██████████| 487/487 [58:53<00:00,  7.26s/it]\n",
            "x21: 100%|██████████| 510/510 [59:49<00:00,  7.04s/it]\n",
            "x16: 100%|██████████| 515/515 [59:54<00:00,  6.98s/it]\n",
            "x03: 100%|██████████| 465/465 [59:58<00:00,  7.74s/it]\n",
            "x05: 100%|██████████| 505/505 [1:00:14<00:00,  7.16s/it]\n",
            "x30: 100%|██████████| 511/511 [1:00:30<00:00,  7.11s/it]\n",
            "x26: 100%|██████████| 520/520 [1:00:55<00:00,  7.03s/it]\n",
            "x01: 100%|██████████| 523/523 [1:01:02<00:00,  7.00s/it]\n",
            "x25: 100%|██████████| 510/510 [1:01:06<00:00,  7.19s/it]\n",
            "x08: 100%|██████████| 517/517 [1:01:22<00:00,  7.12s/it]\n",
            "x27: 100%|██████████| 498/498 [1:01:41<00:00,  7.43s/it]\n",
            "x10: 100%|██████████| 510/510 [1:01:42<00:00,  7.26s/it]\n",
            "x28: 100%|██████████| 495/495 [1:01:46<00:00,  7.49s/it]\n",
            "x09: 100%|██████████| 508/508 [1:01:50<00:00,  7.30s/it]\n",
            "x15: 100%|██████████| 498/498 [1:01:53<00:00,  7.46s/it]\n",
            "x32: 100%|██████████| 538/538 [1:01:55<00:00,  6.91s/it]\n",
            "x02: 100%|██████████| 469/469 [1:02:01<00:00,  7.93s/it]\n",
            "x20: 100%|██████████| 513/513 [1:02:04<00:00,  7.26s/it]\n",
            "x23: 100%|██████████| 527/527 [1:02:08<00:00,  7.07s/it]\n",
            "x13: 100%|██████████| 506/506 [1:02:10<00:00,  7.37s/it]\n",
            "x14: 100%|██████████| 490/490 [1:02:15<00:00,  7.62s/it]\n",
            "x31: 100%|██████████| 557/557 [1:02:14<00:00,  6.70s/it]\n",
            "x12: 100%|██████████| 527/527 [1:02:40<00:00,  7.14s/it]\n"
          ]
        }
      ],
      "source": [
        "answers = {}\n",
        "# with open(os.path.join(\"event-2-answers.txt\"), \"r\") as f:\n",
        "with open(os.path.join(\"event-2-answers.txt\"), \"r\") as f:\n",
        "    for answer in f.read().split(\"\\n\\n\"):\n",
        "        answers[answer[:3]] = list(\"\".join(answer.split()[2::2]))\n",
        "\n",
        "names = [\n",
        "    \"x01\", \"x02\", \"x03\", \"x04\", \"x05\", \"x06\", \"x07\", \"x08\", \"x09\", \"x10\",\n",
        "    \"x11\", \"x12\", \"x13\", \"x14\", \"x15\", \"x16\", \"x17\", \"x18\", \"x19\", \"x20\",\n",
        "    \"x21\", \"x22\", \"x23\", \"x24\", \"x25\", \"x26\", \"x27\", \"x28\", \"x29\", \"x30\",\n",
        "    \"x31\", \"x32\", \"x33\", \"x34\", \"x35\"\n",
        "]\n",
        "\n",
        "o_test = []\n",
        "y_test = []\n",
        "groups_test = []\n",
        "print(\"Testing...\")\n",
        "with ProcessPoolExecutor(max_workers=num_worker) as executor:\n",
        "    task_list = []\n",
        "    for i in range(len(names)):\n",
        "        labels = answers[names[i]]\n",
        "        task_list.append(executor.submit(worker, names[i], labels))\n",
        "\n",
        "    for task in as_completed(task_list):\n",
        "        X, y, groups = task.result()\n",
        "        o_test.extend(X)\n",
        "        y_test.extend(y)\n",
        "        groups_test.extend(groups)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving features into a pickle file"
      ],
      "metadata": {
        "id": "AQIQaGj6AE3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBlWaxvmo3GH",
        "outputId": "7eae3dc0-da5d-470f-9fae-537c4d2f1d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ok!\n"
          ]
        }
      ],
      "source": [
        "apnea_ecg = dict(o_train=o_train, y_train=y_train, groups_train=groups_train, o_test=o_test, y_test=y_test,\n",
        "                  groups_test=groups_test)\n",
        "with open(os.path.join(\"apnea-ecg.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(apnea_ecg, f, protocol=2)\n",
        "\n",
        "print(\"\\nok!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the proposed model (E‐MSCNN)"
      ],
      "metadata": {
        "id": "5hmq4AskAMem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"NOTES: Batch data is different each time in keras, which result in slight differences in results.\"\"\"\n",
        "import pickle\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dropout, MaxPooling1D, Reshape, multiply, Conv1D, GlobalAveragePooling1D, Dense, Input\n",
        "from keras.models import Model, load_model\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from scipy.interpolate import splev, splrep\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import random\n",
        "\n",
        "base_dir = \"apnea-ecg-database-1.0.0\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "ir = 3  # interpolate interval\n",
        "before = 2\n",
        "after = 2\n",
        "# normalize\n",
        "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
        "\n",
        "def load_data(path):\n",
        "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
        "    # with open(os.path.join(base_dir, path), 'rb') as f:\n",
        "    with open(os.path.join(path), 'rb') as f:  # read preprocessing result\n",
        "        apnea_ecg = pickle.load(f)\n",
        "    x_train1,x_train2,x_train3 = [],[],[]\n",
        "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
        "    groups_train = apnea_ecg[\"groups_train\"]\n",
        "    for i in range(len(o_train)):\n",
        "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_train[i]\n",
        "        # Curve interpolation\n",
        "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
        "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
        "        x_train1.append([rri_interp_signal, ampl_interp_signal])  # 5-minute-long segment\n",
        "        x_train2.append([rri_interp_signal[180:720], ampl_interp_signal[180:720]])  # 3-minute-long segment\n",
        "        x_train3.append([rri_interp_signal[360:540], ampl_interp_signal[360:540]])  # 1-minute-long segment\n",
        "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
        "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
        "\n",
        "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
        "    num = [i for i in range(16709)]\n",
        "    vallist = set(num) - set(trainlist)\n",
        "    vallist = list(vallist)\n",
        "    for i in trainlist:\n",
        "        x_training1.append(x_train1[i])\n",
        "        x_training2.append(x_train2[i])\n",
        "        x_training3.append(x_train3[i])\n",
        "        y_training.append(y_train[i])\n",
        "        groups_training.append(groups_train[i])\n",
        "    for i in vallist:\n",
        "        x_val1.append(x_train1[i])\n",
        "        x_val2.append(x_train2[i])\n",
        "        x_val3.append(x_train3[i])\n",
        "        y_val.append(y_train[i])\n",
        "        groups_val.append(groups_train[i])\n",
        "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    y_training = np.array(y_training, dtype=\"float32\")\n",
        "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    y_val = np.array(y_val, dtype=\"float32\")\n",
        "    x_test1,x_test2,x_test3 = [],[],[]\n",
        "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
        "    groups_test = apnea_ecg[\"groups_test\"]\n",
        "    for i in range(len(o_test)):\n",
        "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_test[i]\n",
        "        # Curve interpolation\n",
        "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
        "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
        "        x_test1.append([rri_interp_signal, ampl_interp_signal])\n",
        "        x_test2.append([rri_interp_signal[180:720], ampl_interp_signal[180:720]])\n",
        "        x_test3.append([rri_interp_signal[360:540], ampl_interp_signal[360:540]])\n",
        "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
        "    y_test = np.array(y_test, dtype=\"float32\")\n",
        "\n",
        "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, \\\n",
        "           x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test\n",
        "\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch > 70 and (epoch - 1) % 10 == 0:\n",
        "        lr *= 0.1\n",
        "    print(\"Learning rate: \", lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "# Setting up the architecture\n",
        "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3):\n",
        "    # SA-CNN-3\n",
        "    input1 = Input(shape=input_a_shape)\n",
        "    x1 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input1)\n",
        "    x1 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x1)\n",
        "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
        "    x1 = Conv1D(32, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x1)\n",
        "    x1 = MaxPooling1D(pool_size=5, padding=\"same\")(x1)\n",
        "\n",
        "    # SA-CNN-2\n",
        "    input2 = Input(shape=input_b_shape)\n",
        "    x2 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input2)\n",
        "    x2 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x2)\n",
        "    x2 = MaxPooling1D(pool_size=3, padding=\"same\")(x2)\n",
        "    x2 = Conv1D(32, kernel_size=11, strides=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x2)\n",
        "\n",
        "    # SA-CNN-1\n",
        "    input3 = Input(shape=input_c_shape)\n",
        "    x3 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
        "    x3 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x3)\n",
        "    x3 = MaxPooling1D(pool_size=3, padding=\"same\")(x3)\n",
        "    x3 = Conv1D(32, kernel_size=1, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x3)\n",
        "\n",
        "    # Channel-wise attention module\n",
        "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)\n",
        "    squeeze = GlobalAveragePooling1D()(concat)\n",
        "    excitation = Dense(48, activation='relu')(squeeze)\n",
        "    excitation = Dense(96, activation='sigmoid')(excitation)\n",
        "    excitation = Reshape((1, 96))(excitation)\n",
        "    scale = multiply([concat, excitation])\n",
        "    x = GlobalAveragePooling1D()(scale)\n",
        "    dp = Dropout(0.5)(x)\n",
        "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
        "    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "_PyTXUiDEKJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "Ulq7fIcvEsjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load_data\n",
        "path = \"apnea-ecg.pkl\"\n",
        "x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2,\\\n",
        "x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test = load_data(path)\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=2)  # Convert to two categories\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
        "print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
      ],
      "metadata": {
        "id": "l-a6ZL4qEicG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "0I0b-OzYE4cD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFOE38JArZcF",
        "outputId": "f7f2ea2d-1c7a-4c27-9ee6-bd19e9a973fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_shape (11696, 900, 2) (11696, 540, 2) (11696, 180, 2)\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 1/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.9278 - accuracy: 0.6585\n",
            "Epoch 1: val_accuracy improved from -inf to 0.79812, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 43s 416ms/step - loss: 0.9278 - accuracy: 0.6585 - val_loss: 0.7002 - val_accuracy: 0.7981 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 2/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.6351 - accuracy: 0.8210\n",
            "Epoch 2: val_accuracy improved from 0.79812 to 0.85059, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 37s 409ms/step - loss: 0.6351 - accuracy: 0.8210 - val_loss: 0.5454 - val_accuracy: 0.8506 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 3/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.8393\n",
            "Epoch 3: val_accuracy did not improve from 0.85059\n",
            "92/92 [==============================] - 34s 367ms/step - loss: 0.5572 - accuracy: 0.8393 - val_loss: 0.5249 - val_accuracy: 0.8422 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 4/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.5103 - accuracy: 0.8505\n",
            "Epoch 4: val_accuracy improved from 0.85059 to 0.86296, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 35s 382ms/step - loss: 0.5103 - accuracy: 0.8505 - val_loss: 0.4657 - val_accuracy: 0.8630 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 5/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.4736 - accuracy: 0.8600\n",
            "Epoch 5: val_accuracy improved from 0.86296 to 0.86994, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 36s 395ms/step - loss: 0.4736 - accuracy: 0.8600 - val_loss: 0.4375 - val_accuracy: 0.8699 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 6/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8686\n",
            "Epoch 6: val_accuracy improved from 0.86994 to 0.88590, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 33s 360ms/step - loss: 0.4440 - accuracy: 0.8686 - val_loss: 0.4042 - val_accuracy: 0.8859 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 7/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.8720\n",
            "Epoch 7: val_accuracy did not improve from 0.88590\n",
            "92/92 [==============================] - 34s 371ms/step - loss: 0.4329 - accuracy: 0.8720 - val_loss: 0.4250 - val_accuracy: 0.8773 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 8/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.8812\n",
            "Epoch 8: val_accuracy improved from 0.88590 to 0.89088, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 34s 375ms/step - loss: 0.4072 - accuracy: 0.8812 - val_loss: 0.3792 - val_accuracy: 0.8909 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 9/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3999 - accuracy: 0.8824\n",
            "Epoch 9: val_accuracy did not improve from 0.89088\n",
            "92/92 [==============================] - 32s 345ms/step - loss: 0.3999 - accuracy: 0.8824 - val_loss: 0.3775 - val_accuracy: 0.8877 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 10/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.8866\n",
            "Epoch 10: val_accuracy improved from 0.89088 to 0.89607, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 32s 345ms/step - loss: 0.3811 - accuracy: 0.8866 - val_loss: 0.3550 - val_accuracy: 0.8961 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 11/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.8898\n",
            "Epoch 11: val_accuracy did not improve from 0.89607\n",
            "92/92 [==============================] - 34s 364ms/step - loss: 0.3763 - accuracy: 0.8898 - val_loss: 0.3571 - val_accuracy: 0.8937 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 12/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.8932\n",
            "Epoch 12: val_accuracy improved from 0.89607 to 0.89807, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 34s 367ms/step - loss: 0.3591 - accuracy: 0.8932 - val_loss: 0.3485 - val_accuracy: 0.8981 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 13/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3499 - accuracy: 0.8960\n",
            "Epoch 13: val_accuracy did not improve from 0.89807\n",
            "92/92 [==============================] - 36s 393ms/step - loss: 0.3499 - accuracy: 0.8960 - val_loss: 0.3408 - val_accuracy: 0.8973 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 14/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3538 - accuracy: 0.8932\n",
            "Epoch 14: val_accuracy did not improve from 0.89807\n",
            "92/92 [==============================] - 33s 362ms/step - loss: 0.3538 - accuracy: 0.8932 - val_loss: 0.3742 - val_accuracy: 0.8789 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 15/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.8921\n",
            "Epoch 15: val_accuracy improved from 0.89807 to 0.90445, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 36s 391ms/step - loss: 0.3486 - accuracy: 0.8921 - val_loss: 0.3239 - val_accuracy: 0.9044 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 16/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8948\n",
            "Epoch 16: val_accuracy did not improve from 0.90445\n",
            "92/92 [==============================] - 37s 400ms/step - loss: 0.3488 - accuracy: 0.8948 - val_loss: 0.3333 - val_accuracy: 0.8945 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 17/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3292 - accuracy: 0.8992\n",
            "Epoch 17: val_accuracy did not improve from 0.90445\n",
            "92/92 [==============================] - 37s 396ms/step - loss: 0.3292 - accuracy: 0.8992 - val_loss: 0.3153 - val_accuracy: 0.9044 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 18/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3311 - accuracy: 0.8958\n",
            "Epoch 18: val_accuracy improved from 0.90445 to 0.90624, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 36s 388ms/step - loss: 0.3311 - accuracy: 0.8958 - val_loss: 0.3131 - val_accuracy: 0.9062 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 19/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.8983\n",
            "Epoch 19: val_accuracy did not improve from 0.90624\n",
            "92/92 [==============================] - 36s 388ms/step - loss: 0.3230 - accuracy: 0.8983 - val_loss: 0.3071 - val_accuracy: 0.9060 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 20/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3188 - accuracy: 0.8983\n",
            "Epoch 20: val_accuracy improved from 0.90624 to 0.90824, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 36s 389ms/step - loss: 0.3188 - accuracy: 0.8983 - val_loss: 0.3109 - val_accuracy: 0.9082 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 21/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.9012\n",
            "Epoch 21: val_accuracy did not improve from 0.90824\n",
            "92/92 [==============================] - 33s 359ms/step - loss: 0.3171 - accuracy: 0.9012 - val_loss: 0.3154 - val_accuracy: 0.8969 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 22/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3190 - accuracy: 0.8987\n",
            "Epoch 22: val_accuracy did not improve from 0.90824\n",
            "92/92 [==============================] - 36s 387ms/step - loss: 0.3190 - accuracy: 0.8987 - val_loss: 0.3080 - val_accuracy: 0.9074 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 23/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8994\n",
            "Epoch 23: val_accuracy did not improve from 0.90824\n",
            "92/92 [==============================] - 36s 388ms/step - loss: 0.3132 - accuracy: 0.8994 - val_loss: 0.3011 - val_accuracy: 0.9068 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 24/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.9035\n",
            "Epoch 24: val_accuracy did not improve from 0.90824\n",
            "92/92 [==============================] - 33s 360ms/step - loss: 0.3040 - accuracy: 0.9035 - val_loss: 0.2979 - val_accuracy: 0.9042 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 25/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.9024\n",
            "Epoch 25: val_accuracy did not improve from 0.90824\n",
            "92/92 [==============================] - 33s 360ms/step - loss: 0.3071 - accuracy: 0.9024 - val_loss: 0.3085 - val_accuracy: 0.9005 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 26/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.9053\n",
            "Epoch 26: val_accuracy did not improve from 0.90824\n",
            "92/92 [==============================] - 33s 358ms/step - loss: 0.3019 - accuracy: 0.9053 - val_loss: 0.3027 - val_accuracy: 0.8981 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 27/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3018 - accuracy: 0.9044\n",
            "Epoch 27: val_accuracy improved from 0.90824 to 0.90944, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 34s 370ms/step - loss: 0.3018 - accuracy: 0.9044 - val_loss: 0.2885 - val_accuracy: 0.9094 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 28/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.9078\n",
            "Epoch 28: val_accuracy improved from 0.90944 to 0.91123, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 33s 358ms/step - loss: 0.2957 - accuracy: 0.9078 - val_loss: 0.2845 - val_accuracy: 0.9112 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 29/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.9048\n",
            "Epoch 29: val_accuracy did not improve from 0.91123\n",
            "92/92 [==============================] - 34s 366ms/step - loss: 0.3004 - accuracy: 0.9048 - val_loss: 0.3028 - val_accuracy: 0.8975 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 30/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.9055\n",
            "Epoch 30: val_accuracy did not improve from 0.91123\n",
            "92/92 [==============================] - 34s 372ms/step - loss: 0.3007 - accuracy: 0.9055 - val_loss: 0.2900 - val_accuracy: 0.9108 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 31/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2976 - accuracy: 0.9077\n",
            "Epoch 31: val_accuracy improved from 0.91123 to 0.91183, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 34s 375ms/step - loss: 0.2976 - accuracy: 0.9077 - val_loss: 0.2981 - val_accuracy: 0.9118 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 32/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.9089\n",
            "Epoch 32: val_accuracy did not improve from 0.91183\n",
            "92/92 [==============================] - 33s 362ms/step - loss: 0.2934 - accuracy: 0.9089 - val_loss: 0.2827 - val_accuracy: 0.9088 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 33/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.9068\n",
            "Epoch 33: val_accuracy did not improve from 0.91183\n",
            "92/92 [==============================] - 34s 370ms/step - loss: 0.2919 - accuracy: 0.9068 - val_loss: 0.2793 - val_accuracy: 0.9110 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 34/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9101\n",
            "Epoch 34: val_accuracy did not improve from 0.91183\n",
            "92/92 [==============================] - 34s 373ms/step - loss: 0.2871 - accuracy: 0.9101 - val_loss: 0.2902 - val_accuracy: 0.9060 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 35/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9101\n",
            "Epoch 35: val_accuracy did not improve from 0.91183\n",
            "92/92 [==============================] - 32s 349ms/step - loss: 0.2850 - accuracy: 0.9101 - val_loss: 0.3200 - val_accuracy: 0.8901 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 36/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2826 - accuracy: 0.9109\n",
            "Epoch 36: val_accuracy improved from 0.91183 to 0.91462, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 33s 354ms/step - loss: 0.2826 - accuracy: 0.9109 - val_loss: 0.2701 - val_accuracy: 0.9146 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 37/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9142\n",
            "Epoch 37: val_accuracy improved from 0.91462 to 0.91682, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 34s 370ms/step - loss: 0.2771 - accuracy: 0.9142 - val_loss: 0.2707 - val_accuracy: 0.9168 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 38/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9128\n",
            "Epoch 38: val_accuracy did not improve from 0.91682\n",
            "92/92 [==============================] - 32s 351ms/step - loss: 0.2785 - accuracy: 0.9128 - val_loss: 0.2733 - val_accuracy: 0.9092 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 39/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9120\n",
            "Epoch 39: val_accuracy improved from 0.91682 to 0.91801, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 34s 373ms/step - loss: 0.2777 - accuracy: 0.9120 - val_loss: 0.2681 - val_accuracy: 0.9180 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 40/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9124\n",
            "Epoch 40: val_accuracy did not improve from 0.91801\n",
            "92/92 [==============================] - 32s 350ms/step - loss: 0.2779 - accuracy: 0.9124 - val_loss: 0.3078 - val_accuracy: 0.8913 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 41/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9108\n",
            "Epoch 41: val_accuracy improved from 0.91801 to 0.91821, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 32s 350ms/step - loss: 0.2796 - accuracy: 0.9108 - val_loss: 0.2714 - val_accuracy: 0.9182 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 42/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9130\n",
            "Epoch 42: val_accuracy did not improve from 0.91821\n",
            "92/92 [==============================] - 32s 350ms/step - loss: 0.2779 - accuracy: 0.9130 - val_loss: 0.2699 - val_accuracy: 0.9148 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 43/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.9133\n",
            "Epoch 43: val_accuracy did not improve from 0.91821\n",
            "92/92 [==============================] - 32s 351ms/step - loss: 0.2732 - accuracy: 0.9133 - val_loss: 0.2700 - val_accuracy: 0.9170 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 44/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9123\n",
            "Epoch 44: val_accuracy did not improve from 0.91821\n",
            "92/92 [==============================] - 32s 346ms/step - loss: 0.2729 - accuracy: 0.9123 - val_loss: 0.2651 - val_accuracy: 0.9130 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 45/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9178\n",
            "Epoch 45: val_accuracy did not improve from 0.91821\n",
            "92/92 [==============================] - 32s 342ms/step - loss: 0.2668 - accuracy: 0.9178 - val_loss: 0.2627 - val_accuracy: 0.9148 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 46/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9117\n",
            "Epoch 46: val_accuracy did not improve from 0.91821\n",
            "92/92 [==============================] - 35s 380ms/step - loss: 0.2751 - accuracy: 0.9117 - val_loss: 0.2618 - val_accuracy: 0.9154 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 47/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9165\n",
            "Epoch 47: val_accuracy did not improve from 0.91821\n",
            "92/92 [==============================] - 33s 358ms/step - loss: 0.2662 - accuracy: 0.9165 - val_loss: 0.2604 - val_accuracy: 0.9148 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 48/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2611 - accuracy: 0.9158\n",
            "Epoch 48: val_accuracy did not improve from 0.91821\n",
            "92/92 [==============================] - 33s 355ms/step - loss: 0.2611 - accuracy: 0.9158 - val_loss: 0.3501 - val_accuracy: 0.8767 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 49/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9138\n",
            "Epoch 49: val_accuracy improved from 0.91821 to 0.91841, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 33s 354ms/step - loss: 0.2662 - accuracy: 0.9138 - val_loss: 0.2624 - val_accuracy: 0.9184 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 50/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9190\n",
            "Epoch 50: val_accuracy improved from 0.91841 to 0.91961, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 33s 353ms/step - loss: 0.2607 - accuracy: 0.9190 - val_loss: 0.2596 - val_accuracy: 0.9196 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 51/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9190\n",
            "Epoch 51: val_accuracy did not improve from 0.91961\n",
            "92/92 [==============================] - 32s 342ms/step - loss: 0.2607 - accuracy: 0.9190 - val_loss: 0.3131 - val_accuracy: 0.8947 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 52/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9178\n",
            "Epoch 52: val_accuracy did not improve from 0.91961\n",
            "92/92 [==============================] - 32s 341ms/step - loss: 0.2655 - accuracy: 0.9178 - val_loss: 0.2750 - val_accuracy: 0.9078 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 53/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9178\n",
            "Epoch 53: val_accuracy did not improve from 0.91961\n",
            "92/92 [==============================] - 32s 343ms/step - loss: 0.2605 - accuracy: 0.9178 - val_loss: 0.2621 - val_accuracy: 0.9190 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 54/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9206\n",
            "Epoch 54: val_accuracy did not improve from 0.91961\n",
            "92/92 [==============================] - 32s 343ms/step - loss: 0.2578 - accuracy: 0.9206 - val_loss: 0.2568 - val_accuracy: 0.9180 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 55/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9211\n",
            "Epoch 55: val_accuracy did not improve from 0.91961\n",
            "92/92 [==============================] - 35s 378ms/step - loss: 0.2549 - accuracy: 0.9211 - val_loss: 0.2540 - val_accuracy: 0.9174 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 56/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9187\n",
            "Epoch 56: val_accuracy improved from 0.91961 to 0.92081, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 34s 366ms/step - loss: 0.2550 - accuracy: 0.9187 - val_loss: 0.2666 - val_accuracy: 0.9208 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 57/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9241\n",
            "Epoch 57: val_accuracy did not improve from 0.92081\n",
            "92/92 [==============================] - 33s 354ms/step - loss: 0.2498 - accuracy: 0.9241 - val_loss: 0.2546 - val_accuracy: 0.9188 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 58/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9189\n",
            "Epoch 58: val_accuracy improved from 0.92081 to 0.92380, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 32s 342ms/step - loss: 0.2579 - accuracy: 0.9189 - val_loss: 0.2515 - val_accuracy: 0.9238 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 59/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2470 - accuracy: 0.9235\n",
            "Epoch 59: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 31s 340ms/step - loss: 0.2470 - accuracy: 0.9235 - val_loss: 0.2531 - val_accuracy: 0.9146 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 60/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9225\n",
            "Epoch 60: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 34s 369ms/step - loss: 0.2498 - accuracy: 0.9225 - val_loss: 0.2671 - val_accuracy: 0.9106 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 61/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9216\n",
            "Epoch 61: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 33s 355ms/step - loss: 0.2482 - accuracy: 0.9216 - val_loss: 0.2557 - val_accuracy: 0.9202 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 62/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.9219\n",
            "Epoch 62: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 33s 354ms/step - loss: 0.2456 - accuracy: 0.9219 - val_loss: 0.2707 - val_accuracy: 0.9142 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 63/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9237\n",
            "Epoch 63: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 33s 353ms/step - loss: 0.2455 - accuracy: 0.9237 - val_loss: 0.2527 - val_accuracy: 0.9192 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 64/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.9243\n",
            "Epoch 64: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 31s 339ms/step - loss: 0.2441 - accuracy: 0.9243 - val_loss: 0.2431 - val_accuracy: 0.9236 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 65/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9231\n",
            "Epoch 65: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 31s 338ms/step - loss: 0.2436 - accuracy: 0.9231 - val_loss: 0.2479 - val_accuracy: 0.9228 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 66/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9278\n",
            "Epoch 66: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 32s 343ms/step - loss: 0.2363 - accuracy: 0.9278 - val_loss: 0.2544 - val_accuracy: 0.9214 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 67/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.9243\n",
            "Epoch 67: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 32s 347ms/step - loss: 0.2400 - accuracy: 0.9243 - val_loss: 0.2468 - val_accuracy: 0.9190 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 68/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.9259\n",
            "Epoch 68: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 32s 349ms/step - loss: 0.2364 - accuracy: 0.9259 - val_loss: 0.2562 - val_accuracy: 0.9174 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 69/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9286\n",
            "Epoch 69: val_accuracy did not improve from 0.92380\n",
            "92/92 [==============================] - 32s 350ms/step - loss: 0.2339 - accuracy: 0.9286 - val_loss: 0.2598 - val_accuracy: 0.9160 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 70/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9239\n",
            "Epoch 70: val_accuracy improved from 0.92380 to 0.92579, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 35s 378ms/step - loss: 0.2407 - accuracy: 0.9239 - val_loss: 0.2379 - val_accuracy: 0.9258 - lr: 0.0010\n",
            "Learning rate:  0.0010000000474974513\n",
            "Epoch 71/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9294\n",
            "Epoch 71: val_accuracy did not improve from 0.92579\n",
            "92/92 [==============================] - 32s 349ms/step - loss: 0.2357 - accuracy: 0.9294 - val_loss: 0.2429 - val_accuracy: 0.9208 - lr: 0.0010\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 72/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9339\n",
            "Epoch 72: val_accuracy improved from 0.92579 to 0.92819, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 35s 379ms/step - loss: 0.2197 - accuracy: 0.9339 - val_loss: 0.2374 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 73/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.9353\n",
            "Epoch 73: val_accuracy did not improve from 0.92819\n",
            "92/92 [==============================] - 32s 354ms/step - loss: 0.2203 - accuracy: 0.9353 - val_loss: 0.2349 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 74/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9351\n",
            "Epoch 74: val_accuracy improved from 0.92819 to 0.92859, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 32s 351ms/step - loss: 0.2155 - accuracy: 0.9351 - val_loss: 0.2342 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 75/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.9375\n",
            "Epoch 75: val_accuracy did not improve from 0.92859\n",
            "92/92 [==============================] - 35s 380ms/step - loss: 0.2143 - accuracy: 0.9375 - val_loss: 0.2334 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 76/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.9368\n",
            "Epoch 76: val_accuracy did not improve from 0.92859\n",
            "92/92 [==============================] - 35s 377ms/step - loss: 0.2177 - accuracy: 0.9368 - val_loss: 0.2433 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 77/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.9353\n",
            "Epoch 77: val_accuracy did not improve from 0.92859\n",
            "92/92 [==============================] - 35s 374ms/step - loss: 0.2166 - accuracy: 0.9353 - val_loss: 0.2363 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 78/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9366\n",
            "Epoch 78: val_accuracy did not improve from 0.92859\n",
            "92/92 [==============================] - 31s 341ms/step - loss: 0.2158 - accuracy: 0.9366 - val_loss: 0.2374 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 79/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.9365\n",
            "Epoch 79: val_accuracy did not improve from 0.92859\n",
            "92/92 [==============================] - 34s 368ms/step - loss: 0.2177 - accuracy: 0.9365 - val_loss: 0.2332 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 80/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.9356\n",
            "Epoch 80: val_accuracy improved from 0.92859 to 0.92938, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 32s 343ms/step - loss: 0.2157 - accuracy: 0.9356 - val_loss: 0.2327 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
            "Learning rate:  0.00010000000474974513\n",
            "Epoch 81/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9357\n",
            "Epoch 81: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 33s 363ms/step - loss: 0.2164 - accuracy: 0.9357 - val_loss: 0.2336 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
            "Learning rate:  1.0000000474974514e-05\n",
            "Epoch 82/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9365\n",
            "Epoch 82: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 33s 356ms/step - loss: 0.2134 - accuracy: 0.9365 - val_loss: 0.2323 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 83/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2144 - accuracy: 0.9370\n",
            "Epoch 83: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 33s 353ms/step - loss: 0.2144 - accuracy: 0.9370 - val_loss: 0.2320 - val_accuracy: 0.9276 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 84/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9381\n",
            "Epoch 84: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 32s 352ms/step - loss: 0.2120 - accuracy: 0.9381 - val_loss: 0.2319 - val_accuracy: 0.9286 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 85/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9374\n",
            "Epoch 85: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 32s 350ms/step - loss: 0.2125 - accuracy: 0.9374 - val_loss: 0.2323 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 86/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.9360\n",
            "Epoch 86: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 32s 351ms/step - loss: 0.2121 - accuracy: 0.9360 - val_loss: 0.2320 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 87/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9372\n",
            "Epoch 87: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 32s 352ms/step - loss: 0.2112 - accuracy: 0.9372 - val_loss: 0.2319 - val_accuracy: 0.9280 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 88/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9380\n",
            "Epoch 88: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 33s 352ms/step - loss: 0.2115 - accuracy: 0.9380 - val_loss: 0.2322 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 89/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9373\n",
            "Epoch 89: val_accuracy did not improve from 0.92938\n",
            "92/92 [==============================] - 32s 352ms/step - loss: 0.2114 - accuracy: 0.9373 - val_loss: 0.2320 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 90/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9383\n",
            "Epoch 90: val_accuracy improved from 0.92938 to 0.92958, saving model to weights.best.hdf5\n",
            "92/92 [==============================] - 32s 344ms/step - loss: 0.2125 - accuracy: 0.9383 - val_loss: 0.2323 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-05\n",
            "Epoch 91/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9384\n",
            "Epoch 91: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 32s 343ms/step - loss: 0.2110 - accuracy: 0.9384 - val_loss: 0.2320 - val_accuracy: 0.9286 - lr: 1.0000e-05\n",
            "Learning rate:  1.0000000656873453e-06\n",
            "Epoch 92/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.9372\n",
            "Epoch 92: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 31s 339ms/step - loss: 0.2119 - accuracy: 0.9372 - val_loss: 0.2319 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 93/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9377\n",
            "Epoch 93: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 35s 376ms/step - loss: 0.2123 - accuracy: 0.9377 - val_loss: 0.2319 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 94/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.9403\n",
            "Epoch 94: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 31s 340ms/step - loss: 0.2101 - accuracy: 0.9403 - val_loss: 0.2319 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 95/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9386\n",
            "Epoch 95: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 32s 348ms/step - loss: 0.2114 - accuracy: 0.9386 - val_loss: 0.2320 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 96/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9392\n",
            "Epoch 96: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 35s 379ms/step - loss: 0.2099 - accuracy: 0.9392 - val_loss: 0.2320 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 97/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9402\n",
            "Epoch 97: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 33s 363ms/step - loss: 0.2108 - accuracy: 0.9402 - val_loss: 0.2319 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 98/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9387\n",
            "Epoch 98: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 32s 345ms/step - loss: 0.2098 - accuracy: 0.9387 - val_loss: 0.2320 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 99/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9391\n",
            "Epoch 99: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 33s 359ms/step - loss: 0.2104 - accuracy: 0.9391 - val_loss: 0.2320 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
            "Learning rate:  1.0000001111620804e-06\n",
            "Epoch 100/100\n",
            "92/92 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.9367\n",
            "Epoch 100: val_accuracy did not improve from 0.92958\n",
            "92/92 [==============================] - 33s 355ms/step - loss: 0.2121 - accuracy: 0.9367 - val_loss: 0.2319 - val_accuracy: 0.9284 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "filepath = 'weights.best.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "callbacks_list = [lr_scheduler, checkpoint]\n",
        "history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
        "                    validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verifying perfomance with testing dataset"
      ],
      "metadata": {
        "id": "qcv5nemPA5fG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-r6Sqg4uSO8"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "filepath = './weights.best.hdf5'\n",
        "model = load_model(filepath)\n",
        "loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
        "# save prediction score\n",
        "y_score = model.predict([x_test1, x_test2, x_test3])\n",
        "output = pd.DataFrame({\"y_true\": y_test[:, 1], \"y_score\": y_score[:, 1], \"subject\": groups_test})\n",
        "output.to_csv(\"SE-MSCNN.csv\", index=False)\n",
        "y_true, y_pred = np.argmax(y_test, axis=-1), np.argmax(model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1), axis=-1)\n",
        "C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
        "TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
        "acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "print(\"acc: {}, sn: {}, sp: {}, f1: {}\".format(acc, sn, sp, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing with other baseline models"
      ],
      "metadata": {
        "id": "xqQnY49EDD01"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5431ZS2QiSOQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "\n",
        "base_dir = \"output\"\n",
        "\n",
        "# Table 2\n",
        "output = []\n",
        "svm = '/content/SVM.csv'\n",
        "lr = '/content/LR.csv'\n",
        "knn = '/content/KNN.csv'\n",
        "mlp = '/content/MLP.csv'\n",
        "se_mscnn = '/content/SE-MSCNN.csv'\n",
        "methods = [svm, lr, knn, mlp, se_mscnn]\n",
        "for method in methods:\n",
        "    df = pd.read_csv(method, header=0)\n",
        "    df[\"y_pred\"] = df[\"y_score\"] > 0.5\n",
        "    df = df.groupby(by=\"subject\").apply(lambda d: d[\"y_pred\"].mean() * 60)\n",
        "    df.name = method\n",
        "    output.append(df)\n",
        "output = pd.concat(output, axis=1)\n",
        "\n",
        "with open(\"additional-information.txt\", \"r\") as f:\n",
        "    original = []\n",
        "    for line in f:\n",
        "        rows = line.strip().split(\"\\t\")\n",
        "        if len(rows) == 12:\n",
        "            if rows[0].startswith(\"x\"):\n",
        "                original.append([rows[0], float(rows[3]) / float(rows[1]) * 60])\n",
        "original = pd.DataFrame(original, columns=[\"subject\", \"original\"])\n",
        "original = original.set_index(\"subject\")\n",
        "all = pd.concat((output, original), axis=1)\n",
        "corr = all.corr()\n",
        "all1 = all.applymap(lambda a: int(a > 5))\n",
        "result = []\n",
        "\n",
        "# Iteratively calculate confusion matrix\n",
        "for method in methods:\n",
        "    C = confusion_matrix(all1[\"original\"], all1[method], labels=(1, 0))\n",
        "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
        "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
        "    auc = roc_auc_score(all[\"original\"] > 5, all[method])\n",
        "    result.append([method, acc * 100, sn * 100, sp * 100, auc, corr[\"original\"][method]])\n",
        "\n",
        "    # a pictorial representation (heatmap) of the confusion matrix\n",
        "    plt.figure()\n",
        "    plt.imshow(C, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix for {method}')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(2)\n",
        "    plt.xticks(tick_marks, [1, 0])\n",
        "    plt.yticks(tick_marks, [1, 0])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, f'{C[i, j]}', horizontalalignment='center', color='white' if C[i, j] > C.max() / 2 else 'black')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}